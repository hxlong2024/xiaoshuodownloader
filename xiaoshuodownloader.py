import streamlit as st
import requests
from bs4 import BeautifulSoup
import os
import re
import zipfile


# ==========================================
# 1. æ ¸å¿ƒçˆ¬è™«é€»è¾‘ (ç›´æ¥å¤ç”¨ï¼Œå®Œå…¨ä¸ç”¨æ”¹)
# ==========================================
class JJJXSW_Engine:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Linux; Android 10; Mobile) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Mobile Safari/537.36",
            "Referer": "https://m.jjjxsw.com/"
        }
        self.base_url = "https://m.jjjxsw.com"

    def run(self, keyword):
        """
        è¿”å›: (æ–‡ä»¶å, æ–‡ä»¶äºŒè¿›åˆ¶å†…å®¹) æˆ–è€… (None, None)
        """
        log_msgs = []

        def log(msg):
            log_msgs.append(msg)

        try:
            # 1. æœç´¢
            log("ğŸ” [1/4] æ­£åœ¨æœç´¢...")
            search_url = f"{self.base_url}/e/search/index.php"
            data = {"keyboard": keyword, "Submit22": "æœç´¢", "show": "title"}
            resp = requests.post(search_url, data=data, headers=self.headers)
            resp.encoding = 'utf-8'
            soup = BeautifulSoup(resp.text, 'html.parser')

            result_items = soup.select(".booklist_a .list_a .main")
            if not result_items:
                return None, None, log_msgs + ["âŒ æœç´¢æ— ç»“æœ"]

            main_div = result_items[0]
            link_tag = main_div.find('a')
            raw_title = link_tag.get_text().strip()
            intro_href = link_tag['href']

            # æå–ä½œè€…
            author = "ä½šå"
            for span in main_div.find_all('span'):
                if "ä½œè€…" in span.get_text():
                    author = span.get_text().replace("ä½œè€…ï¼š", "").replace("ä½œè€…:", "").strip()
                    break
            log(f"ğŸ“– é”å®š: ã€Š{raw_title}ã€‹ ä½œè€…: {author}")

            # 2. ä»‹ç»é¡µ
            intro_url = self.base_url + intro_href
            headers_intro = self.headers.copy()
            headers_intro['Referer'] = search_url
            intro_resp = requests.get(intro_url, headers=headers_intro)
            intro_resp.encoding = 'utf-8'
            intro_soup = BeautifulSoup(intro_resp.text, 'html.parser')

            # 3. å¯»æ‰¾ç¡®è®¤é¡µ
            confirm_url = None
            sso_area = intro_soup.select_one(".sso_d")
            if sso_area:
                for link in sso_area.find_all('a'):
                    if "ä¸‹è½½" in link.get_text() or "txt" in link.get_text().lower():
                        confirm_url = link.get('href')
                        break
            if not confirm_url:
                t = intro_soup.find('a', string=re.compile("ä¸‹è½½"))
                if t: confirm_url = t['href']

            if not confirm_url: return None, None, log_msgs + ["âŒ æœªæ‰¾åˆ°ä¸‹è½½å…¥å£"]
            if not confirm_url.startswith("http"): confirm_url = self.base_url + confirm_url

            # 4. è§£æçœŸå®åœ°å€
            headers_confirm = self.headers.copy()
            headers_confirm['Referer'] = intro_url
            confirm_resp = requests.get(confirm_url, headers=headers_confirm)
            confirm_soup = BeautifulSoup(confirm_resp.text, 'html.parser')

            target_link = confirm_soup.find('a', id='id0')
            if not target_link:
                target_link = confirm_soup.find('a', href=re.compile(r'doaction\.php'))

            if not target_link: return None, None, log_msgs + ["âŒ æ— æ³•è§£æ doaction é“¾æ¥"]

            real_url = target_link['href']
            if not real_url.startswith("http"): real_url = self.base_url + real_url

            # 5. ä¸‹è½½å†…å®¹åˆ°å†…å­˜
            log("â¬‡ï¸ [4/4] æ­£åœ¨ä¸‹è½½æ–‡ä»¶æµ...")
            headers_file = self.headers.copy()
            headers_file['Referer'] = confirm_url

            file_resp = requests.get(real_url, headers=headers_file)  # ä¸ä½¿ç”¨streamï¼Œç›´æ¥è¯»å…¥å†…å­˜

            if file_resp.status_code == 200:
                clean_title = re.sub(r'[\\/*?:"<>|]', "", raw_title)
                clean_author = re.sub(r'[\\/*?:"<>|]', "", author)
                filename = f"{clean_title} by {clean_author}.txt"
                log("âœ… ä¸‹è½½æˆåŠŸï¼")
                return filename, file_resp.content, log_msgs
            else:
                return None, None, log_msgs + [f"âŒ HTTPé”™è¯¯: {file_resp.status_code}"]

        except Exception as e:
            return None, None, log_msgs + [f"âŒ å¼‚å¸¸: {e}"]


# ==========================================
# 2. Streamlit ç½‘é¡µç•Œé¢
# ==========================================

st.set_page_config(page_title="å°è¯´ä¸‹è½½å™¨", page_icon="ğŸ“š")

st.title("ğŸ“š æ‰‹æœºå°è¯´ä¸‹è½½åŠ©æ‰‹")
st.write("åœ¨æ‰‹æœºä¸Šè¾“å…¥ä¹¦åï¼Œç”µè„‘å¸®ä½ è·‘è…¿ä¸‹è½½ã€‚")

# è¾“å…¥æ¡†
keyword = st.text_input("è¾“å…¥å°è¯´åç§°", placeholder="ä¾‹å¦‚ï¼šæ’æ˜Ÿæ—¶åˆ»")

# æŒ‰é’®
if st.button("å¼€å§‹æœç´¢å¹¶ä¸‹è½½", type="primary"):
    if not keyword:
        st.warning("è¯·è¾“å…¥åç§°ï¼")
    else:
        engine = JJJXSW_Engine()

        # æ˜¾ç¤ºè¿›åº¦æ¡
        with st.spinner('æ­£åœ¨ç”µè„‘åå°ç–¯ç‹‚è¿è¡Œä¸­...'):
            filename, file_content, logs = engine.run(keyword)

        # æ˜¾ç¤ºæ—¥å¿—
        with st.expander("æŸ¥çœ‹è¿è¡Œæ—¥å¿—"):
            for msg in logs:
                st.write(msg)

        # ç»“æœå¤„ç†
        if filename and file_content:
            st.success(f"æˆåŠŸæ‰¾åˆ°ï¼š{filename}")

            # --- æ ¸å¿ƒåŠŸèƒ½ï¼šæä¾›ç»™æ‰‹æœºä¸‹è½½ ---
            # 1. ä¸‹è½½ TXT
            st.download_button(
                label="ğŸ“¥ ç‚¹å‡»ä¸‹è½½ TXT åˆ°æ‰‹æœº",
                data=file_content,
                file_name=filename,
                mime="text/plain"
            )

            # 2. å‹ç¼©å¹¶ä¸‹è½½ ZIP
            # åœ¨å†…å­˜ä¸­åˆ›å»ºZIP
            import io

            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "a", zipfile.ZIP_DEFLATED, False) as zip_file:
                zip_file.writestr(filename, file_content)

            st.download_button(
                label="ğŸ“¦ ç‚¹å‡»ä¸‹è½½ ZIP åˆ°æ‰‹æœº",
                data=zip_buffer.getvalue(),
                file_name=filename.replace(".txt", ".zip"),
                mime="application/zip"
            )
        else:
            st.error("ä¸‹è½½å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ä¸Šæ–¹æ—¥å¿—ã€‚")