import streamlit as st
import aiohttp
import asyncio
from bs4 import BeautifulSoup
import re
import zipfile
import io
import time
import urllib.parse
import mimetypes


# ==========================================
# 1. åŸºç¡€å¼•æ“
# ==========================================

class BaseEngine:
    def __init__(self):
        self.source_name = "æœªçŸ¥æº"
        # æ¨¡æ‹Ÿæœ€æ–° Chrome
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
        }

    def log(self, msgs, text):
        msgs.append(f"[{self.source_name}] {text}")

    def clean_filename(self, text):
        return re.sub(r'[\\/*?:"<>|]', "", text).strip()

    def validate_title(self, user_keyword, site_title):
        def clean(s): return re.sub(r'[^\w\u4e00-\u9fa5]', '', s).lower()

        return clean(user_keyword) in clean(site_title)

    async def run(self, session, keyword):
        raise NotImplementedError


# ==========================================
# 2. 99å°è¯´ç½‘ (ä¿æŒåŸæ · - æä¾›æ–‡ä»¶ä¸‹è½½)
# ==========================================
class JJJXSW_Engine(BaseEngine):
    def __init__(self):
        super().__init__()
        self.source_name = "99å°è¯´ç½‘"
        self.base_url = "https://m.jjjxsw.com"
        self.headers["Referer"] = "https://m.jjjxsw.com/"

    async def run(self, session, keyword):
        logs = []
        try:
            self.log(logs, f"ğŸš€ æœç´¢: {keyword}")
            async with session.post(f"{self.base_url}/e/search/index.php",
                                    data={"keyboard": keyword, "Submit22": "æœç´¢", "show": "title"},
                                    headers=self.headers) as resp:
                soup = BeautifulSoup(await resp.text(encoding='utf-8', errors='ignore'), 'html.parser')

            target_item = None;
            target_title = "";
            target_href = "";
            target_author = "ä½šå"
            for item in soup.select(".booklist_a .list_a .main"):
                link = item.find('a')
                if not link: continue
                raw_title = link.get_text().strip()
                if self.validate_title(keyword, raw_title):
                    target_item = item;
                    target_title = raw_title;
                    target_href = link['href']
                    for span in item.find_all('span'):
                        if "ä½œè€…" in span.get_text(): target_author = span.get_text().split(":")[-1].strip(); break
                    break

            if not target_item: return False, None, logs
            self.log(logs, f"âœ… åŒ¹é…: ã€Š{target_title}ã€‹")

            async with session.get(self.base_url + target_href, headers=self.headers) as resp:
                intro_soup = BeautifulSoup(await resp.text(encoding='utf-8', errors='ignore'), 'html.parser')
            confirm_url = None
            sso = intro_soup.select_one(".sso_d")
            if sso:
                for a in sso.find_all('a'):
                    if "ä¸‹è½½" in a.get_text(): confirm_url = a['href']; break
            if not confirm_url:
                t = intro_soup.find('a', string=re.compile("ä¸‹è½½"))
                if t: confirm_url = t['href']
            if not confirm_url: return False, None, logs
            if not confirm_url.startswith("http"): confirm_url = self.base_url + confirm_url

            async with session.get(confirm_url, headers=self.headers) as resp:
                confirm_soup = BeautifulSoup(await resp.text(encoding='utf-8', errors='ignore'), 'html.parser')
            real_link = confirm_soup.find('a', id='id0') or confirm_soup.find('a', href=re.compile(r'doaction\.php'))
            if not real_link: return False, None, logs
            real_url = real_link['href']
            if not real_url.startswith("http"): real_url = self.base_url + real_url

            self.log(logs, "â¬‡ï¸ ä¸‹è½½ä¸­...")
            async with session.get(real_url, headers=self.headers) as resp:
                if resp.status == 200:
                    content = await resp.read()
                    fname = f"{self.clean_filename(target_title)} by {self.clean_filename(target_author)}.txt"
                    # æ³¨æ„ï¼šè¿™é‡Œè¿”å› contentï¼Œè¡¨ç¤ºæ˜¯æ–‡ä»¶æµ
                    return True, {"filename": fname, "author": target_author, "content": content}, logs
            return False, None, logs
        except Exception as e:
            self.log(logs, f"âŒ å¼‚å¸¸: {e}");
            return False, None, logs


# ==========================================
# 3. 00å°è¯´ç½‘ (ç»ˆæä¿®å¤ï¼šå…ˆè®¿é—®ä¸»é¡µæ‹¿Cookie)
# ==========================================
class ZeroShu_Engine(BaseEngine):
    def __init__(self):
        super().__init__()
        self.source_name = "00å°è¯´ç½‘"
        # å¼ºåˆ¶ä½¿ç”¨ httpï¼Œé¿å¼€ https è¯ä¹¦é—®é¢˜
        self.base_url = "http://m.00shu.la" 
        
        # æ¨¡æ‹Ÿæ™®é€šç”µè„‘æµè§ˆå™¨çš„å¤´
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "http://m.00shu.la/",
            "Origin": "http://m.00shu.la",
            "Content-Type": "application/x-www-form-urlencoded"
        }

    async def run(self, session, keyword):
        logs = []
        try:
            # === ç¬¬ä¸€æ­¥ï¼šå…ˆè®¿é—®é¦–é¡µï¼Œä¸ºäº†è·å– Cookie ===
            # å¾ˆå¤šç½‘ç«™é˜²çˆ¬è™«ç­–ç•¥æ˜¯ï¼šæ²¡æœ‰é¦–é¡µçš„ Cookieï¼Œå°±ä¸å…è®¸æœç´¢
            try:
                await session.get(self.base_url, headers=self.headers)
            except:
                pass # å°±ç®—é¦–é¡µæ…¢ï¼Œä¹Ÿå°è¯•ç»§ç»­ï¼Œä¸‡ä¸€ä¸éœ€è¦å‘¢

            # === ç¬¬äºŒæ­¥ï¼šå¸¦ç€ Cookie å»æœç´¢ ===
            self.log(logs, f"ğŸš€ æœç´¢: {keyword}")
            async with session.post(f"{self.base_url}/s.php", 
                                    data={"searchkey": keyword, "type": "articlename"},
                                    headers=self.headers) as resp:
                # 00å°è¯´ç½‘æœ‰æ—¶è¿”å›çš„æ˜¯ä¹±ç ï¼Œå°è¯•ç”¨ gbk æˆ– utf-8 è§£ç 
                content = await resp.read()
                # å°è¯•è‡ªåŠ¨æ£€æµ‹ç¼–ç ï¼Œé€šå¸¸æ˜¯ utf-8
                html = content.decode('utf-8', errors='ignore')
                soup = BeautifulSoup(html, 'html.parser')
            
            target_title = ""; target_href = ""; target_author = "ä½šå"; found = False
            
            # æ‰“å°ä¸€ä¸‹æ‰¾åˆ°å¤šå°‘ä¸ªç»“æœï¼Œæ–¹ä¾¿è°ƒè¯•
            items = soup.select(".searchresult .sone")
            
            for item in items:
                a = item.find('a')
                if not a: continue
                raw_title = a.get_text().strip()
                
                # éªŒè¯æ ‡é¢˜
                if self.validate_title(keyword, raw_title):
                    target_title = raw_title
                    target_href = a['href']
                    span = item.find('span', class_='author')
                    if span: target_author = span.get_text().strip()
                    found = True
                    break

            if not found: 
                # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œæœ‰æ—¶å€™æ˜¯å› ä¸ºç½‘ç«™æŠŠä½ é‡å®šå‘åˆ°äº†è¯¦æƒ…é¡µï¼ˆå¦‚æœæ˜¯å”¯ä¸€ç»“æœï¼‰
                # æ£€æŸ¥æ˜¯ä¸æ˜¯ç›´æ¥è·³åˆ°äº†ä¹¦åé¡µ
                meta_title = soup.select_one("meta[property='og:title']")
                if meta_title and self.validate_title(keyword, meta_title['content']):
                     # è¿™é‡Œå¤„ç†ä¸€ä¸‹å”¯ä¸€ç»“æœç›´æ¥è·³è½¬çš„æƒ…å†µï¼ˆé¢„ç•™é€»è¾‘ï¼Œé€šå¸¸00shuä¸ä¼šï¼‰
                     pass
                
                self.log(logs, "âŒ æœªæ‰¾åˆ° (æˆ–è¢«åçˆ¬æ‹¦æˆª)")
                return False, None, logs
                
            self.log(logs, f"âœ… åŒ¹é…: ã€Š{target_title}ã€‹")

            # === ç¬¬ä¸‰æ­¥ï¼šå¤„ç†è¯¦æƒ…é¡µé“¾æ¥ ===
            # è¡¥å…¨é“¾æ¥
            if target_href.startswith("/"):
                detail_url = self.base_url + target_href
            elif not target_href.startswith("http"):
                detail_url = f"{self.base_url}/{target_href}"
            else:
                detail_url = target_href
            
            # å¼ºåˆ¶ http
            detail_url = detail_url.replace("https://", "http://")
            
            async with session.get(detail_url, headers=self.headers) as resp:
                detail_soup = BeautifulSoup(await resp.text(errors='ignore'), 'html.parser')
            
            # === ç¬¬å››æ­¥ï¼šæ‰¾ä¸‹è½½ä¹Ÿ ===
            inter_href = None
            btn_list = detail_soup.find(id="btnlist")
            if btn_list:
                l = btn_list.find('a', string=re.compile("ä¸‹è½½"))
                if l: inter_href = l['href']
            
            if not inter_href: return False, None, logs
            
            # è¡¥å…¨ä¸‹è½½é¡µé“¾æ¥
            inter_url = urllib.parse.urljoin(detail_url, inter_href)
            inter_url = inter_url.replace("https://", "http://")

            async with session.get(inter_url, headers=self.headers) as resp:
                down_soup = BeautifulSoup(await resp.text(errors='ignore'), 'html.parser')
            
            # === ç¬¬äº”æ­¥ï¼šæ‰¾æ–‡ä»¶é“¾æ¥ ===
            file_link = down_soup.find('a', href=re.compile(r'\.(txt|zip|rar)$', re.IGNORECASE))
            if not file_link: file_link = down_soup.find('a', string=re.compile("ä¸‹è½½"), href=lambda h: h and ('txt' in h or 'down' in h))
            
            if not file_link: return False, None, logs
            real_url = file_link['href']
            real_url = urllib.parse.urljoin(inter_url, real_url)
            real_url = real_url.replace("https://", "http://")

            self.log(logs, "â¬‡ï¸ ä¸‹è½½ä¸­...")
            async with session.get(real_url, headers=self.headers) as resp:
                if resp.status == 200:
                    content = await resp.read()
                    ext = ".txt"
                    if content[:2] == b'PK': ext = ".zip"
                    elif content[:2] == b'Rar': ext = ".rar"
                    fname = f"{self.clean_filename(target_title)} by {self.clean_filename(target_author)}{ext}"
                    return True, {"filename": fname, "author": target_author, "content": content}, logs
            return False, None, logs
        except Exception as e:
            self.log(logs, f"âŒ å¼‚å¸¸: {e}")
            return False, None, logs


# ==========================================
# 4. Z-Library å¼•æ“ (V9.0 ç›´è¾¾è¯¦æƒ…é¡µç‰ˆ)
# ==========================================

class ZLibrary_Engine(BaseEngine):
    def __init__(self, email, password):
        super().__init__()
        self.source_name = "Z-Library"
        self.base_url = "https://en.zlib.li"
        self.email = email
        self.password = password

    async def login(self, session, logs):
        if not self.email: return False
        self.log(logs, "ğŸ”‘ æ­£åœ¨ç™»å½•...")
        try:
            h = self.headers.copy();
            h["Origin"] = self.base_url;
            h["Referer"] = f"{self.base_url}/login"
            payload = {"email": self.email, "password": self.password, "site_mode": "books", "action": "login",
                       "redirectUrl": self.base_url + "/"}
            async with session.post(f"{self.base_url}/", data=payload, headers=h) as resp:
                text = await resp.text()
                if 'id="loginForm"' in text or "validation-error" in text:
                    self.log(logs, "âŒ ç™»å½•å¤±è´¥")
                    return False
                self.log(logs, "ğŸ”“ ç™»å½•æˆåŠŸ")
                return True
        except Exception as e:
            self.log(logs, f"âŒ ç™»å½•å¼‚å¸¸: {e}");
            return False

    async def run(self, session, keyword):
        logs = []
        if not await self.login(session, logs): return False, None, logs

        try:
            # 1. æœç´¢
            self.log(logs, f"ğŸš€ æœç´¢: {keyword}")
            async with session.get(f"{self.base_url}/s/", params={"q": keyword}, headers=self.headers) as resp:
                soup = BeautifulSoup(await resp.text(errors='ignore'), 'html.parser')

            target_item = None
            target_data = {}

            # è§£ææœç´¢ç»“æœ
            for item in soup.find_all('z-bookcard'):
                t_div = item.find('div', slot='title')
                title = t_div.get_text().strip() if t_div else ""
                href = item.get('href')

                if self.validate_title(keyword, title) and href:
                    target_item = item
                    target_data = {"title": title, "href": href.strip()}
                    a_div = item.find('div', slot='author')
                    target_data['author'] = a_div.get_text().strip() if a_div else "ä½šå"
                    break

            if not target_item:
                self.log(logs, "âŒ æœªæ‰¾åˆ°åŒ¹é…ä¹¦ç±")
                return False, None, logs

            # 2. è·å–è¯¦æƒ…é¡µé“¾æ¥å¹¶è¿”å›
            # å¼ºåˆ¶æ‹¼æ¥å®Œæ•´ URLï¼Œç¡®ä¿æ˜¯ https://...
            detail_url = urllib.parse.urljoin(self.base_url, target_data['href'])

            self.log(logs, f"âœ… é”å®š: ã€Š{target_data['title']}ã€‹")
            self.log(logs, f"ğŸ”— ç”Ÿæˆè¯¦æƒ…é¡µé“¾æ¥: {detail_url}")

            # === ä¿®æ”¹å¤„ï¼šä¸å†ä¸‹è½½ï¼Œç›´æ¥è¿”å› URL ===
            # æˆ‘ä»¬è¿”å›ä¸€ä¸ªç‰¹æ®Šçš„å­—å…¸ï¼Œæ²¡æœ‰ 'content' å­—æ®µï¼Œä½†æœ‰ 'url'
            return True, {
                "type": "link",  # æ ‡è®°è¿™æ˜¯ä¸ªé“¾æ¥
                "title": target_data['title'],
                "author": target_data['author'],
                "url": detail_url
            }, logs

        except Exception as e:
            self.log(logs, f"âŒ å¼‚å¸¸: {e}");
            return False, None, logs


# ==========================================
# 5. UI éƒ¨åˆ† (é€‚é…é“¾æ¥æ˜¾ç¤º)
# ==========================================
async def search_race_mode(keyword, zlib_creds):
    engines = [JJJXSW_Engine()]    #, ZeroShu_Engine()
    if zlib_creds['email']: engines.append(ZLibrary_Engine(zlib_creds['email'], zlib_creds['password']))

    start = time.time()
    all_logs = []

    # ================= ä¿®æ”¹å¼€å§‹ =================
    # 1. è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º 15 ç§’ (é˜²æ­¢ç½‘ç«™æ…¢å¯¼è‡´æŠ¥é”™)
    timeout = aiohttp.ClientTimeout(total=15)
    
    # 2. å¿½ç•¥ SSL è¯ä¹¦éªŒè¯ (å¾ˆå¤šå°è¯´ç«™è¯ä¹¦æ˜¯è¿‡æœŸçš„ï¼Œè®¾ä¸º False å¯ä»¥å¼ºåˆ¶è¿æ¥)
    connector = aiohttp.TCPConnector(ssl=False)

    async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
    # ================= ä¿®æ”¹ç»“æŸ =================
    
        tasks = [asyncio.create_task(e.run(session, keyword)) for e in engines]
        for t, e in zip(tasks, engines): t.set_name(e.source_name)

        pending = set(tasks)
        while pending:
            done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)
            for task in done:
                success, result, logs = await task
                all_logs.extend(logs)
                if success and result:
                    for p in pending: p.cancel()
                    return {"success": True, "source": task.get_name(), "data": result, "logs": all_logs,
                            "time": time.time() - start}
    return {"success": False, "logs": all_logs, "time": time.time() - start}



st.set_page_config(page_title="å…¨èƒ½èµ›é©¬ä¸‹è½½å™¨", page_icon="ğŸ¦„", layout="centered")
st.markdown(
    """
    <style>
    /* 1. æ ¸å¿ƒä»£ç ï¼šå‡å°‘é¡¶éƒ¨ç©ºç™½ */
    .block-container {
        padding-top: 0rem !important;  /* æ•°å­—è¶Šå°ï¼Œç¦»é¡¶éƒ¨è¶Šè¿‘ï¼Œé»˜è®¤å¤§æ¦‚æ˜¯ 5rem */
        padding-bottom: 1rem !important;
    }

    /* 2. ä½ åŸæœ¬çš„æŒ‰é’®å’Œæç¤ºæ¡†æ ·å¼ */
    .stButton>button{width:100%;border-radius:8px;font-weight:bold}
    .success-box{padding:15px;background:#e6fffa;border:1px solid #38b2ac;color:#234e52;border-radius:8px}
    .link-box{padding:15px;background:#ebf8ff;border:1px solid #4299e1;color:#2b6cb0;border-radius:8px;text-align:center;}
    .link-box a {color: #2b6cb0; font-weight: bold; font-size: 1.2em; text-decoration: none;}
    </style>
    """,
    unsafe_allow_html=True)


st.title("")
st.caption("å¹¶å‘æ£€ç´¢ï¼š99å°è¯´ | 00å°è¯´ | Z-Library (æä¾›è¯¦æƒ…é¡µç›´é“¾)")

with st.sidebar:
    st.header("ğŸ”‘ Z-Library")
    z_email = st.text_input("Email");
    z_pass = st.text_input("Password", type="password")

keyword = st.text_input("ä¹¦å", placeholder="ä¾‹å¦‚ï¼šå¯æ€œçš„ç¤¾ç•œ")
if st.button("ğŸš€ æé€Ÿæ£€ç´¢", type="primary"):
    if not keyword:
        st.warning("è¯·è¾“å…¥ä¹¦å")
    else:
        st.info("ğŸ” å…¨ç½‘å¹¶å‘æ£€ç´¢ä¸­...")
        res = asyncio.run(search_race_mode(keyword, {'email': z_email, 'password': z_pass}))

        if res["success"]:
            d = res['data']

            # === åˆ†æ”¯åˆ¤æ–­ï¼šæ˜¯ç›´æ¥ä¸‹è½½çš„æ–‡ä»¶ï¼Œè¿˜æ˜¯ ZLib çš„é“¾æ¥ï¼Ÿ ===

            # æƒ…å†µ A: è¿™æ˜¯ä¸€ä¸ªé“¾æ¥ (Z-Library)
            if d.get("type") == "link":
                st.markdown(
                    f"""
                    <div class='link-box'>
                        <h3>ğŸ•µï¸â€â™‚ï¸ å·²æ‰¾åˆ°ä¹¦ç±è¯¦æƒ…é¡µ</h3>
                        <p><b>{d['title']}</b><br>ä½œè€…: {d['author']}</p>
                        <hr style="margin:10px 0; border:0; border-top:1px solid #bbeeef;">
                        <p>è¯·ç‚¹å‡»ä¸‹æ–¹é“¾æ¥å»æµè§ˆå™¨æ‰‹åŠ¨ä¸‹è½½ï¼š</p>
                        <a href="{d['url']}" target="_blank">ğŸ‘‰ ç‚¹å‡»æ‰“å¼€: {d['title']} ğŸ‘ˆ</a>
                    </div>
                    """,
                    unsafe_allow_html=True
                )
                # é¢å¤–æä¾›ä¸€ä¸ªå¤åˆ¶æ¡†ï¼Œæ–¹ä¾¿å¤åˆ¶
                st.text_input("æˆ–å¤åˆ¶æ­¤é“¾æ¥:", d['url'])
                st.caption(f"æ¥æº: {res['source']} (è€—æ—¶ {res['time']:.2f}s)")

            # æƒ…å†µ B: è¿™æ˜¯ä¸€ä¸ªæ–‡ä»¶ (å…¶ä»–å°è¯´ç½‘)
            elif "content" in d:
                st.markdown(
                    f"<div class='success-box'><h3>âœ… æ–‡ä»¶è·å–æˆåŠŸ!</h3><b>{d['filename']}</b><br>æº: {res['source']} ({res['time']:.2f}s)</div>",
                    unsafe_allow_html=True)

                mime = "application/octet-stream"
                if d['filename'].endswith(".pdf"):
                    mime = "application/pdf"
                elif d['filename'].endswith(".epub"):
                    mime = "application/epub+zip"
                elif d['filename'].endswith(".txt"):
                    mime = "text/plain"

                c1, c2 = st.columns(2)
                c1.download_button(f"ğŸ“¥ ä¸‹è½½ ({d['filename'].split('.')[-1]})", d['content'], d['filename'], mime=mime)

                buf = io.BytesIO()
                with zipfile.ZipFile(buf, "a", zipfile.ZIP_DEFLATED, False) as zf:
                    zf.writestr(d['filename'], d['content'])
                c2.download_button("ğŸ“¦ ä¸‹è½½ZIP", buf.getvalue(), d['filename'] + ".zip", "application/zip")

        else:
            st.error("ğŸ˜­ å…¨ç½‘æœªæ‰¾åˆ°èµ„æº")

        with st.expander("æŸ¥çœ‹æ‰§è¡Œæ—¥å¿—"):

            for m in res["logs"]: st.text(m)






