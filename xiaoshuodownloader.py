import streamlit as st
import aiohttp
import asyncio
from bs4 import BeautifulSoup
import re
import zipfile
import io
import time
import urllib.parse


# ==========================================
# 1. åŸºç¡€å¼•æ“æ¶æ„ (å‡çº§ç‰ˆï¼šå¸¦ä¹¦åæ ¸å¯¹)
# ==========================================

class BaseEngine:
    def __init__(self):
        self.source_name = "æœªçŸ¥æº"
        self.base_url = ""
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Linux; Android 10; Mobile) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Mobile Safari/537.36"
        }

    def log(self, msgs, text):
        msgs.append(f"[{self.source_name}] {text}")

    def clean_filename(self, text):
        return re.sub(r'[\\/*?:"<>|]', "", text).strip()

    def validate_title(self, user_keyword, site_title):
        """
        ä¹¦åæ ¸å¯¹é€»è¾‘ï¼š
        1. å»é™¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
        2. ç¡®ä¿ ç”¨æˆ·è¾“å…¥çš„å…³é”®è¯ åŒ…å«åœ¨ ç½‘ç«™æ ‡é¢˜ ä¸­
        """

        # åªä¿ç•™æ±‰å­—ã€å­—æ¯ã€æ•°å­—
        def clean(s):
            return re.sub(r'[^\w\u4e00-\u9fa5]', '', s).lower()

        kw = clean(user_keyword)
        st = clean(site_title)

        # æ ¸å¿ƒé€»è¾‘ï¼šç½‘ç«™æ ‡é¢˜å¿…é¡»åŒ…å«ç”¨æˆ·æœçš„è¯ (æˆ–è€…å®Œå…¨ç›¸ç­‰)
        # æ¯”å¦‚æœ "å…ƒå°Š"ï¼Œç»“æœ "å…ƒå°Š(ç²¾æ ¡ç‰ˆ)" -> é€šè¿‡
        # æ¯”å¦‚æœ "å…ƒå°Š"ï¼Œç»“æœ "æ–—ç ´è‹ç©¹" -> å¤±è´¥
        is_match = kw in st
        return is_match

    async def run(self, session, keyword):
        raise NotImplementedError


# ==========================================
# 2. 99å°è¯´ç½‘ å¼•æ“ (å¸¦æ ¡éªŒ)
# ==========================================

class JJJXSW_Engine(BaseEngine):
    def __init__(self):
        super().__init__()
        self.source_name = "99å°è¯´ç½‘"
        self.base_url = "https://m.jjjxsw.com"
        self.headers["Referer"] = "https://m.jjjxsw.com/"

    async def run(self, session, keyword):
        logs = []
        try:
            self.log(logs, f"ğŸš€ æœç´¢: {keyword}")
            search_url = f"{self.base_url}/e/search/index.php"
            data = {"keyboard": keyword, "Submit22": "æœç´¢", "show": "title"}

            async with session.post(search_url, data=data, headers=self.headers) as resp:
                text = await resp.text(encoding='utf-8', errors='ignore')
                soup = BeautifulSoup(text, 'html.parser')

            result_items = soup.select(".booklist_a .list_a .main")

            target_item = None
            target_title = ""
            target_href = ""
            target_author = "ä½šå"

            # === å¾ªç¯æ£€æŸ¥æ‰€æœ‰ç»“æœ ===
            for item in result_items:
                link_tag = item.find('a')
                if not link_tag: continue

                raw_title = link_tag.get_text().strip()

                # æ ¸å¯¹ä¹¦å
                if self.validate_title(keyword, raw_title):
                    target_item = item
                    target_title = raw_title
                    target_href = link_tag['href']

                    # æå–ä½œè€…
                    for span in item.find_all('span'):
                        if "ä½œè€…" in span.get_text():
                            target_author = span.get_text().replace("ä½œè€…ï¼š", "").replace("ä½œè€…:", "").strip()
                            break
                    break  # æ‰¾åˆ°åŒ¹é…çš„å°±è·³å‡º
                else:
                    self.log(logs, f"âš ï¸ è·³è¿‡ä¸åŒ¹é…ç»“æœ: {raw_title}")

            if not target_item:
                self.log(logs, "âŒ æœªæ‰¾åˆ°åŒ¹é…ä¹¦åçš„ç»“æœ")
                return False, None, logs

            self.log(logs, f"âœ… åŒ¹é…æˆåŠŸ: ã€Š{target_title}ã€‹")

            # Step 2: ä»‹ç»é¡µ
            intro_url = self.base_url + target_href
            async with session.get(intro_url, headers=self.headers) as resp:
                intro_soup = BeautifulSoup(await resp.text(encoding='utf-8', errors='ignore'), 'html.parser')

            confirm_url = None
            sso_area = intro_soup.select_one(".sso_d")
            if sso_area:
                for link in sso_area.find_all('a'):
                    if "ä¸‹è½½" in link.get_text() or "txt" in link.get_text().lower():
                        confirm_url = link.get('href')
                        break
            if not confirm_url:
                t = intro_soup.find('a', string=re.compile("ä¸‹è½½"))
                if t: confirm_url = t['href']

            if not confirm_url: return False, None, logs
            if not confirm_url.startswith("http"): confirm_url = self.base_url + confirm_url

            # Step 3: è§£æçœŸå®é“¾æ¥
            async with session.get(confirm_url, headers=self.headers) as resp:
                confirm_soup = BeautifulSoup(await resp.text(encoding='utf-8', errors='ignore'), 'html.parser')

            target_link = confirm_soup.find('a', id='id0') or confirm_soup.find('a', href=re.compile(r'doaction\.php'))
            if not target_link: return False, None, logs

            real_url = target_link['href']
            if not real_url.startswith("http"): real_url = self.base_url + real_url

            # Step 4: ä¸‹è½½
            self.log(logs, "â¬‡ï¸ æ‹‰å–æ–‡ä»¶æµ...")
            async with session.get(real_url, headers=self.headers) as resp:
                if resp.status == 200:
                    content = await resp.read()
                    filename = f"{self.clean_filename(target_title)} by {self.clean_filename(target_author)}.txt"
                    return True, {"filename": filename, "author": target_author, "content": content}, logs

            return False, None, logs
        except Exception as e:
            self.log(logs, f"âŒ å¼‚å¸¸: {e}")
            return False, None, logs


# ==========================================
# 3. 00å°è¯´ç½‘ å¼•æ“ (å¸¦æ ¡éªŒ)
# ==========================================

class ZeroShu_Engine(BaseEngine):
    def __init__(self):
        super().__init__()
        self.source_name = "00å°è¯´ç½‘"
        self.base_url = "https://m.00shu.la"

    async def run(self, session, keyword):
        logs = []
        try:
            self.log(logs, f"ğŸš€ æœç´¢: {keyword}")
            search_url = f"{self.base_url}"
            data = {"searchkey": keyword, "type": "articlename"}

            async with session.post(search_url, data=data, headers=self.headers) as resp:
                soup = BeautifulSoup(await resp.text(errors='ignore'), 'html.parser')

            results = soup.select(".searchresult .sone")

            target_title = ""
            target_href = ""
            target_author = "ä½šå"
            found_match = False

            # === å¾ªç¯æ£€æŸ¥æ‰€æœ‰ç»“æœ ===
            for item in results:
                a_tag = item.find('a')
                if not a_tag: continue

                raw_title = a_tag.get_text().strip()

                # æ ¸å¯¹ä¹¦å
                if self.validate_title(keyword, raw_title):
                    target_title = raw_title
                    target_href = a_tag['href']

                    span_auth = item.find('span', class_='author')
                    if span_auth:
                        target_author = span_auth.get_text().strip()

                    found_match = True
                    break  # æ‰¾åˆ°å°±åœ
                else:
                    self.log(logs, f"âš ï¸ è·³è¿‡ä¸åŒ¹é…ç»“æœ: {raw_title}")

            if not found_match:
                self.log(logs, "âŒ æ— åŒ¹é…ä¹¦åçš„ç»“æœ")
                return False, None, logs

            self.log(logs, f"âœ… åŒ¹é…æˆåŠŸ: ã€Š{target_title}ã€‹")

            detail_url = target_href if target_href.startswith("http") else self.base_url + target_href

            # Step 2: è¯¦æƒ…é¡µ
            async with session.get(detail_url, headers=self.headers) as resp:
                detail_soup = BeautifulSoup(await resp.text(errors='ignore'), 'html.parser')

            btn_list = detail_soup.find(id="btnlist")
            intermediate_href = None
            if btn_list:
                link_tag = btn_list.find('a', string=re.compile("ä¸‹è½½"))
                if link_tag: intermediate_href = link_tag['href']

            if not intermediate_href: return False, None, logs
            intermediate_url = intermediate_href if intermediate_href.startswith(
                "http") else self.base_url + intermediate_href

            # Step 3: è§£æçœŸå®æ–‡ä»¶
            async with session.get(intermediate_url, headers=self.headers) as resp:
                down_soup = BeautifulSoup(await resp.text(errors='ignore'), 'html.parser')

            file_link = down_soup.find('a', href=re.compile(r'\.(txt|zip|rar)$', re.IGNORECASE))
            if not file_link:
                file_link = down_soup.find('a', string=re.compile("ä¸‹è½½"),
                                           href=lambda h: h and ('txt' in h or 'down' in h))

            if not file_link: return False, None, logs
            real_file_url = file_link['href']
            if not real_file_url.startswith("http"): real_file_url = urllib.parse.urljoin(intermediate_url,
                                                                                          real_file_url)

            # Step 4: ä¸‹è½½
            self.log(logs, "â¬‡ï¸ æ‹‰å–æ–‡ä»¶æµ...")
            async with session.get(real_file_url, headers=self.headers) as resp:
                if resp.status == 200:
                    content = await resp.read()
                    ext = ".txt"
                    if content[:2] == b'PK':
                        ext = ".zip"
                    elif content[:2] == b'Rar':
                        ext = ".rar"

                    filename = f"{self.clean_filename(target_title)} by {self.clean_filename(target_author)}{ext}"
                    return True, {"filename": filename, "author": target_author, "content": content}, logs

            return False, None, logs
        except Exception as e:
            self.log(logs, f"âŒ å¼‚å¸¸: {e}")
            return False, None, logs


# ==========================================
# 4. èµ›é©¬è°ƒåº¦ (é€»è¾‘ä¸å˜)
# ==========================================

async def search_race_mode(keyword):
    engine_classes = [JJJXSW_Engine, ZeroShu_Engine]
    start_time = time.time()
    all_logs = []

    async with aiohttp.ClientSession() as session:
        tasks = []
        for EngineCls in engine_classes:
            engine = EngineCls()
            task = asyncio.create_task(engine.run(session, keyword))
            task.set_name(engine.source_name)
            tasks.append(task)

        pending = set(tasks)
        while pending:
            done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)
            for task in done:
                success, result, logs = await task
                all_logs.extend(logs)
                if success and result:
                    winner_source = task.get_name()
                    for p_task in pending: p_task.cancel()  # ç†”æ–­å…¶ä»–
                    return {"success": True, "source": winner_source, "data": result, "logs": all_logs,
                            "time": time.time() - start_time}

    return {"success": False, "logs": all_logs, "time": time.time() - start_time}


# ==========================================
# 5. Streamlit ç•Œé¢
# ==========================================
st.set_page_config(page_title="ä¸¥è°¨ç‰ˆèµ›é©¬ä¸‹è½½å™¨", page_icon="ğŸ´", layout="centered")
st.markdown(
    """<style>.stButton>button { width: 100%; border-radius: 8px; font-weight: bold; } .success-box { padding: 15px; background: #e6fffa; border: 1px solid #38b2ac; color: #234e52; border-radius: 8px; margin-bottom: 15px;}</style>""",
    unsafe_allow_html=True)

st.title("ğŸ´ æé€Ÿä¸”ä¸¥è°¨çš„å°è¯´ä¸‹è½½")
st.caption("å¹¶å‘èµ›é©¬ + æ™ºèƒ½ä¹¦åæ ¡éªŒ | æœç»å‡èµ„æº")

keyword = st.text_input("è¾“å…¥ä¹¦å", placeholder="ä¾‹å¦‚ï¼šå…ƒå°Š")

if st.button("ğŸš€ æé€Ÿæœç´¢", type="primary"):
    if not keyword:
        st.warning("è¯·è¾“å…¥ä¹¦åï¼")
    else:
        status_text = st.empty()
        status_text.info("ğŸ” æ­£åœ¨å¹¶å‘æ£€ç´¢å¹¶æ ¸å¯¹ä¹¦å...")

        result = asyncio.run(search_race_mode(keyword))
        status_text.empty()

        if result["success"]:
            data = result['data']
            st.markdown(f"""
            <div class="success-box">
                <h3>âœ… æ ¡éªŒé€šè¿‡ï¼</h3>
                <b>ä¹¦åï¼š</b>{data['filename']}<br>
                <b>æ¥æºï¼š</b>{result['source']} (è€—æ—¶ {result['time']:.2f}s)
            </div>
            """, unsafe_allow_html=True)

            col1, col2 = st.columns(2)
            with col1:
                st.download_button("ğŸ“¥ ä¸‹è½½æ–‡ä»¶", data['content'], file_name=data['filename'])
            with col2:
                zip_buffer = io.BytesIO()
                with zipfile.ZipFile(zip_buffer, "a", zipfile.ZIP_DEFLATED, False) as zf:
                    zf.writestr(data['filename'], data['content'])
                st.download_button("ğŸ“¦ ä¸‹è½½ ZIP", zip_buffer.getvalue(), file_name=data['filename'] + ".zip",
                                   mime="application/zip")
        else:
            st.error("ğŸ˜­ æœªæ‰¾åˆ°åŒ¹é…è¯¥ä¹¦åçš„èµ„æº (å·²è‡ªåŠ¨è¿‡æ»¤ä¸ç›¸å…³ç»“æœ)")

        with st.expander("ğŸ“Š æŸ¥çœ‹æ ¡éªŒæ—¥å¿—"):

            for msg in result["logs"]: st.text(msg)
