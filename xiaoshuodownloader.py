
import streamlit as st
import aiohttp
import asyncio
from bs4 import BeautifulSoup
import re
import zipfile
import io
import time
import urllib.parse
import mimetypes
import datetime  # <--- æ–°å¢ï¼šç”¨äºè®¡ç®—Cookieè¿‡æœŸæ—¶é—´
import extra_streamlit_components as stx  # <--- æ–°å¢ï¼šç”¨äºç®¡ç†Cookie

# ==========================================
# 1. åŸºç¡€å¼•æ“
# ==========================================

class BaseEngine:
    def __init__(self):
        self.source_name = "æœªçŸ¥æº"
        # æ¨¡æ‹Ÿæœ€æ–° Chrome
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
        }

    def log(self, msgs, text):
        msgs.append(f"[{self.source_name}] {text}")

    def clean_filename(self, text):
        return re.sub(r'[\\/*?:"<>|]', "", text).strip()

    def validate_title(self, user_keyword, site_title):
        def clean(s): return re.sub(r'[^\w\u4e00-\u9fa5]', '', s).lower()

        return clean(user_keyword) in clean(site_title)

    async def run(self, session, keyword):
        raise NotImplementedError


# ==========================================
# 2. 99å°è¯´ç½‘ (ä¿æŒåŸæ ·)
# ==========================================
class JJJXSW_Engine(BaseEngine):
    def __init__(self):
        super().__init__()
        self.source_name = "99å°è¯´ç½‘"
        self.base_url = "https://m.jjjxsw.com"
        self.headers["Referer"] = "https://m.jjjxsw.com/"

    async def run(self, session, keyword):
        logs = []
        try:
            self.log(logs, f"ğŸš€ æœç´¢: {keyword}")
            async with session.post(f"{self.base_url}/e/search/index.php",
                                    data={"keyboard": keyword, "Submit22": "æœç´¢", "show": "title"},
                                    headers=self.headers) as resp:
                soup = BeautifulSoup(await resp.text(encoding='utf-8', errors='ignore'), 'html.parser')

            target_item = None;
            target_title = "";
            target_href = "";
            target_author = "ä½šå"
            for item in soup.select(".booklist_a .list_a .main"):
                link = item.find('a')
                if not link: continue
                raw_title = link.get_text().strip()
                if self.validate_title(keyword, raw_title):
                    target_item = item;
                    target_title = raw_title;
                    target_href = link['href']
                    for span in item.find_all('span'):
                        if "ä½œè€…" in span.get_text(): target_author = span.get_text().split(":")[-1].strip(); break
                    break

            if not target_item: return False, None, logs
            self.log(logs, f"âœ… åŒ¹é…: ã€Š{target_title}ã€‹")

            async with session.get(self.base_url + target_href, headers=self.headers) as resp:
                intro_soup = BeautifulSoup(await resp.text(encoding='utf-8', errors='ignore'), 'html.parser')
            confirm_url = None
            sso = intro_soup.select_one(".sso_d")
            if sso:
                for a in sso.find_all('a'):
                    if "ä¸‹è½½" in a.get_text(): confirm_url = a['href']; break
            if not confirm_url:
                t = intro_soup.find('a', string=re.compile("ä¸‹è½½"))
                if t: confirm_url = t['href']
            if not confirm_url: return False, None, logs
            if not confirm_url.startswith("http"): confirm_url = self.base_url + confirm_url

            async with session.get(confirm_url, headers=self.headers) as resp:
                confirm_soup = BeautifulSoup(await resp.text(encoding='utf-8', errors='ignore'), 'html.parser')
            real_link = confirm_soup.find('a', id='id0') or confirm_soup.find('a', href=re.compile(r'doaction\.php'))
            if not real_link: return False, None, logs
            real_url = real_link['href']
            if not real_url.startswith("http"): real_url = self.base_url + real_url

            self.log(logs, "â¬‡ï¸ ä¸‹è½½ä¸­...")
            async with session.get(real_url, headers=self.headers) as resp:
                if resp.status == 200:
                    content = await resp.read()
                    fname = f"{self.clean_filename(target_title)} by {self.clean_filename(target_author)}.txt"
                    return True, {"filename": fname, "author": target_author, "content": content}, logs
            return False, None, logs
        except Exception as e:
            self.log(logs, f"âŒ å¼‚å¸¸: {e}");
            return False, None, logs


# ==========================================
# 3. 00å°è¯´ç½‘ (ä¿ç•™ä»£ç ä½†æš‚ä¸ä½¿ç”¨)
# ==========================================
class ZeroShu_Engine(BaseEngine):
    def __init__(self):
        super().__init__()
        self.source_name = "00å°è¯´ç½‘"
        self.base_url = "http://m.00shu.la" 
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "http://m.00shu.la/",
            "Origin": "http://m.00shu.la",
            "Content-Type": "application/x-www-form-urlencoded"
        }

    async def run(self, session, keyword):
        # ... (æ­¤å¤„çœç•¥å…·ä½“é€»è¾‘ï¼Œä¿æŒä½ ä¹‹å‰çš„ä»£ç ç»“æ„ä¸å˜ï¼Œä¸ºäº†èŠ‚çœç¯‡å¹…æˆ‘æ²¡é‡å¤è´´ä¸­é—´é€»è¾‘ï¼Œç±»ç»“æ„ä¿ç•™å³å¯) ...
        # å¦‚æœä½ ä¹‹å‰è¿™éƒ¨åˆ†æ˜¯å®Œæ•´çš„ï¼Œä¿ç•™å³å¯ï¼Œä¸ç”¨æ”¹åŠ¨
        return False, None, [] 


# ==========================================
# 4. ç¬”è¶£é˜ (æ–°å¢ï¼šä¸“æ²»æ‰¾ä¸åˆ°ä¹¦)
# ==========================================
class BiQuGe_Engine(BaseEngine):
    def __init__(self):
        super().__init__()
        self.source_name = "ç¬”è¶£é˜"
        self.base_url = "https://www.bqgka.com"

    async def run(self, session, keyword):
        logs = []
        try:
            self.log(logs, f"ğŸš€ æœç´¢: {keyword}")
            async with session.get(f"{self.base_url}/s?q={urllib.parse.quote(keyword)}", headers=self.headers) as resp:
                soup = BeautifulSoup(await resp.text(errors='ignore'), 'html.parser')

            target_data = {}
            found = False
            for item in soup.select("h4.bookname a"):
                title = item.get_text().strip()
                if self.validate_title(keyword, title):
                    target_data = {"title": title, "href": item['href'], "author": "æœªçŸ¥"}
                    found = True
                    break
            
            if not found:
                self.log(logs, "âŒ æœªæ‰¾åˆ°")
                return False, None, logs

            self.log(logs, f"âœ… é”å®š: ã€Š{target_data['title']}ã€‹")
            full_url = self.base_url + target_data['href']
            
            return True, {
                "type": "link", 
                "title": target_data['title'],
                "author": target_data['author'],
                "url": full_url
            }, logs
        except Exception as e:
            self.log(logs, f"âŒ å¼‚å¸¸: {e}")
            return False, None, logs


# ==========================================
# 5. Z-Library å¼•æ“
# ==========================================
class ZLibrary_Engine(BaseEngine):
    def __init__(self, email, password):
        super().__init__()
        self.source_name = "Z-Library"
        self.base_url = "https://en.zlib.li"
        self.email = email
        self.password = password

    async def login(self, session, logs):
        if not self.email: return False
        self.log(logs, "ğŸ”‘ æ­£åœ¨ç™»å½•...")
        try:
            h = self.headers.copy();
            h["Origin"] = self.base_url;
            h["Referer"] = f"{self.base_url}/login"
            payload = {"email": self.email, "password": self.password, "site_mode": "books", "action": "login",
                       "redirectUrl": self.base_url + "/"}
            async with session.post(f"{self.base_url}/", data=payload, headers=h) as resp:
                text = await resp.text()
                if 'id="loginForm"' in text or "validation-error" in text:
                    self.log(logs, "âŒ ç™»å½•å¤±è´¥")
                    return False
                self.log(logs, "ğŸ”“ ç™»å½•æˆåŠŸ")
                return True
        except Exception as e:
            self.log(logs, f"âŒ ç™»å½•å¼‚å¸¸: {e}");
            return False

    async def run(self, session, keyword):
        logs = []
        if not await self.login(session, logs): return False, None, logs

        try:
            self.log(logs, f"ğŸš€ æœç´¢: {keyword}")
            async with session.get(f"{self.base_url}/s/", params={"q": keyword}, headers=self.headers) as resp:
                soup = BeautifulSoup(await resp.text(errors='ignore'), 'html.parser')

            target_item = None
            target_data = {}

            for item in soup.find_all('z-bookcard'):
                t_div = item.find('div', slot='title')
                title = t_div.get_text().strip() if t_div else ""
                href = item.get('href')

                if self.validate_title(keyword, title) and href:
                    target_item = item
                    target_data = {"title": title, "href": href.strip()}
                    a_div = item.find('div', slot='author')
                    target_data['author'] = a_div.get_text().strip() if a_div else "ä½šå"
                    break

            if not target_item:
                self.log(logs, "âŒ æœªæ‰¾åˆ°åŒ¹é…ä¹¦ç±")
                return False, None, logs

            detail_url = urllib.parse.urljoin(self.base_url, target_data['href'])
            self.log(logs, f"âœ… é”å®š: ã€Š{target_data['title']}ã€‹")
            self.log(logs, f"ğŸ”— ç”Ÿæˆè¯¦æƒ…é¡µé“¾æ¥: {detail_url}")

            return True, {
                "type": "link", 
                "title": target_data['title'],
                "author": target_data['author'],
                "url": detail_url
            }, logs

        except Exception as e:
            self.log(logs, f"âŒ å¼‚å¸¸: {e}");
            return False, None, logs


# ==========================================
# 6. æœç´¢è°ƒåº¦é€»è¾‘
# ==========================================
async def search_race_mode(keyword, zlib_creds):
    # âœ… ä¿®æ”¹ï¼šåŠ å…¥ BiQuGe_Engineï¼Œæš‚æ—¶æ³¨é‡Šæ‰ ZeroShu_Engine
    engines = [JJJXSW_Engine(), BiQuGe_Engine()] 
    if zlib_creds['email']: engines.append(ZLibrary_Engine(zlib_creds['email'], zlib_creds['password']))

    start = time.time()
    all_logs = []

    timeout = aiohttp.ClientTimeout(total=15)
    connector = aiohttp.TCPConnector(ssl=False)

    async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
        tasks = [asyncio.create_task(e.run(session, keyword)) for e in engines]
        for t, e in zip(tasks, engines): t.set_name(e.source_name)

        pending = set(tasks)
        while pending:
            done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)
            for task in done:
                success, result, logs = await task
                all_logs.extend(logs)
                if success and result:
                    for p in pending: p.cancel()
                    return {"success": True, "source": task.get_name(), "data": result, "logs": all_logs,
                            "time": time.time() - start}
    return {"success": False, "logs": all_logs, "time": time.time() - start}


# ==========================================
# 7. UI éƒ¨åˆ†
# ==========================================

st.set_page_config(page_title="å…¨èƒ½èµ›é©¬ä¸‹è½½å™¨", page_icon="ğŸ¦„", layout="centered")

# åˆå§‹åŒ– Cookie ç®¡ç†å™¨ (æ”¾åœ¨ set_page_config ä¹‹å)
cookie_manager = stx.CookieManager()

st.markdown(
    """
    <style>
    .block-container {
        padding-top: 0rem !important;
        padding-bottom: 1rem !important;
    }
    .stButton>button{width:100%;border-radius:8px;font-weight:bold}
    .success-box{padding:15px;background:#e6fffa;border:1px solid #38b2ac;color:#234e52;border-radius:8px}
    .link-box{padding:15px;background:#ebf8ff;border:1px solid #4299e1;color:#2b6cb0;border-radius:8px;text-align:center;}
    .link-box a {color: #2b6cb0; font-weight: bold; font-size: 1.2em; text-decoration: none;}
    </style>
    """,
    unsafe_allow_html=True)


st.title("")
st.caption("å¹¶å‘æ£€ç´¢ï¼š99å°è¯´ | ç¬”è¶£é˜ | Z-Library")

# === ä¾§è¾¹æ ï¼šCookie è´¦å·ç®¡ç† ===
with st.sidebar:
    st.header("ğŸ”‘ Z-Library")
    
    # 1. è¯»å– Cookie
    cookies = cookie_manager.get_all()
    cookie_email = cookies.get("zlib_email", "") 
    cookie_pass = cookies.get("zlib_pass", "")

    # 2. æ˜¾ç¤ºè¾“å…¥æ¡†
    input_email = st.text_input("Email", value=cookie_email, key="z_email_input")
    input_pass = st.text_input("Password", value=cookie_pass, type="password", key="z_pass_input")

    # 3. ä¿å­˜æŒ‰é’® (30å¤©æœ‰æ•ˆæœŸ)
    if st.button("ğŸ’¾ è®°ä½æˆ‘çš„è´¦å·"):
        expires = datetime.datetime.now() + datetime.timedelta(days=30)
        cookie_manager.set("zlib_email", input_email, expires_at=expires)
        cookie_manager.set("zlib_pass", input_pass, expires_at=expires)
        st.success("å·²ä¿å­˜åˆ°è®¾å¤‡ï¼")
        time.sleep(1)
        st.rerun()

    # 4. æ¸…é™¤æŒ‰é’®
    if st.button("ğŸ—‘ï¸ å¿˜è®°è´¦å·"):
        cookie_manager.delete("zlib_email")
        cookie_manager.delete("zlib_pass")
        st.rerun()

# === ä¸»ç•Œé¢é€»è¾‘ ===
keyword = st.text_input("ä¹¦å", placeholder="ä¾‹å¦‚ï¼šå¯æ€œçš„ç¤¾ç•œ")
if st.button("ğŸš€ æé€Ÿæ£€ç´¢", type="primary"):
    if not keyword:
        st.warning("è¯·è¾“å…¥ä¹¦å")
    else:
        st.info("ğŸ” å…¨ç½‘å¹¶å‘æ£€ç´¢ä¸­...")
        res = asyncio.run(search_race_mode(keyword, {'email': input_email, 'password': input_pass}))

        if res["success"]:
            d = res['data']

            # æƒ…å†µ A: é“¾æ¥ (Z-Lib æˆ– ç¬”è¶£é˜)
            if d.get("type") == "link":
                st.markdown(
                    f"""
                    <div class='link-box'>
                        <h3>ğŸ•µï¸â€â™‚ï¸ å·²æ‰¾åˆ°ä¹¦ç±/è¯¦æƒ…é¡µ</h3>
                        <p><b>{d['title']}</b><br>ä½œè€…: {d['author']}</p>
                        <hr style="margin:10px 0; border:0; border-top:1px solid #bbeeef;">
                        <p>è¯·ç‚¹å‡»ä¸‹æ–¹é“¾æ¥å»æµè§ˆå™¨é˜…è¯»æˆ–ä¸‹è½½ï¼š</p>
                        <a href="{d['url']}" target="_blank">ğŸ‘‰ ç‚¹å‡»æ‰“å¼€: {d['title']} ğŸ‘ˆ</a>
                    </div>
                    """,
                    unsafe_allow_html=True
                )
                st.text_input("æˆ–å¤åˆ¶æ­¤é“¾æ¥:", d['url'])
                st.caption(f"æ¥æº: {res['source']} (è€—æ—¶ {res['time']:.2f}s)")

            # æƒ…å†µ B: æ–‡ä»¶ (99å°è¯´ç½‘)
            elif "content" in d:
                st.markdown(
                    f"<div class='success-box'><h3>âœ… æ–‡ä»¶è·å–æˆåŠŸ!</h3><b>{d['filename']}</b><br>æº: {res['source']} ({res['time']:.2f}s)</div>",
                    unsafe_allow_html=True)

                mime = "application/octet-stream"
                if d['filename'].endswith(".pdf"):
                    mime = "application/pdf"
                elif d['filename'].endswith(".epub"):
                    mime = "application/epub+zip"
                elif d['filename'].endswith(".txt"):
                    mime = "text/plain"

                c1, c2 = st.columns(2)
                c1.download_button(f"ğŸ“¥ ä¸‹è½½ ({d['filename'].split('.')[-1]})", d['content'], d['filename'], mime=mime)

                buf = io.BytesIO()
                with zipfile.ZipFile(buf, "a", zipfile.ZIP_DEFLATED, False) as zf:
                    zf.writestr(d['filename'], d['content'])
                c2.download_button("ğŸ“¦ ä¸‹è½½ZIP", buf.getvalue(), d['filename'] + ".zip", "application/zip")

        else:
            st.error("ğŸ˜­ å…¨ç½‘æœªæ‰¾åˆ°èµ„æº")

        with st.expander("æŸ¥çœ‹æ‰§è¡Œæ—¥å¿—"):
            for m in res["logs"]: st.text(m)

